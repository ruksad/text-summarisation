{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appraoch & Highlights Of This Notebook**\n",
    "\n",
    "Model & Training Startegy For Summarisation - T5 Small Fine Tuning Without PEFT<br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:04:14.506819Z",
     "iopub.status.busy": "2025-10-31T19:04:14.506321Z",
     "iopub.status.idle": "2025-10-31T19:04:25.140922Z",
     "shell.execute_reply": "2025-10-31T19:04:25.140020Z",
     "shell.execute_reply.started": "2025-10-31T19:04:14.506791Z"
    },
    "trusted": true
   },
   "source": [
    "# Install the essential libraries\n",
    "!pip install -U pip\n",
    "!pip install transformers datasets evaluate rouge_score\n",
    "!pip install torch torchvision torchaudio #--index-url https://download.pytorch.org/whl/cu121  # adjust for your CUDA/CPU\n",
    "!pip install transformers datasets peft accelerate evaluate rouge-score nltk sentencepiece\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0\n",
      "MPS available: True\n",
      "MPS built: True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:04:28.033182Z",
     "iopub.status.busy": "2025-10-31T19:04:28.032625Z",
     "iopub.status.idle": "2025-10-31T19:04:42.072590Z",
     "shell.execute_reply": "2025-10-31T19:04:42.071686Z",
     "shell.execute_reply.started": "2025-10-31T19:04:28.033154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This one command downloads, splits, and structures the data\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "# https://huggingface.co/datasets/ccdv/cnn_dailymail\n",
    "\n",
    "# You will now have a 'DatasetDict' with all three splits\n",
    "print(dataset)\n",
    "\n",
    "#small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "#small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "#small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(20000))\n",
    "#small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(100000))\n",
    "small_eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:05:04.392578Z",
     "iopub.status.busy": "2025-10-31T19:05:04.391525Z",
     "iopub.status.idle": "2025-10-31T19:05:29.739587Z",
     "shell.execute_reply": "2025-10-31T19:05:29.738843Z",
     "shell.execute_reply.started": "2025-10-31T19:05:04.392541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:00<00:00, 1990.92it/s, Materializing param=shared.weight]                                                      \n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 1. Choose the model checkpoint\n",
    "model_checkpoint = \"t5-small\"\n",
    "\n",
    "# 2. Load the tokenizer\n",
    "# The tokenizer will turn your text into 'input_ids'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 3. Load the model\n",
    "# T5ForConditionalGeneration is the T5 model for tasks like summarization\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:05:29.741124Z",
     "iopub.status.busy": "2025-10-31T19:05:29.740623Z",
     "iopub.status.idle": "2025-10-31T19:12:05.992926Z",
     "shell.execute_reply": "2025-10-31T19:12:05.992110Z",
     "shell.execute_reply.started": "2025-10-31T19:05:29.741105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:51<00:00, 1951.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# The prefix tells T5 what task to perform\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # 1. Add the prefix to all articles\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "\n",
    "    # 2. Tokenize the articles (our inputs)\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    # 3. Tokenize the highlights (our labels/targets)\n",
    "    # UPDATED: No longer using as_target_tokenizer()\n",
    "    labels= tokenizer(\n",
    "        text_target=examples[\"highlights\"],\n",
    "          max_length=128, \n",
    "          truncation=True)\n",
    "\n",
    "    # 4. Set the 'labels' for the model\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "# Apply this function to all splits in our dataset\n",
    "# 'batched=True' processes multiple examples at once for speed\n",
    "#tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = small_eval_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T19:12:07.547206Z",
     "iopub.status.busy": "2025-10-31T19:12:07.546503Z",
     "iopub.status.idle": "2025-10-31T19:12:07.929844Z",
     "shell.execute_reply": "2025-10-31T19:12:07.929294Z",
     "shell.execute_reply.started": "2025-10-31T19:12:07.547178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/mruksad/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/6e5315f72865c2eaa764c8361360bb938740b9c120a2cf3a7ad218aa0ce452ed (last modified on Wed Feb 11 07:39:47 2026) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# We need to download this for ROUGE to work\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# The function that will be called to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # 'eval_pred' gives us model predictions and the true labels\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # 1. Decode the generated IDs back to text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # 2. Decode the label IDs back to text\n",
    "    # We replace -100 (which are padding tokens)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # 3. Add newlines for ROUGE (it expects summaries to be on separate lines)\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # 4. Compute the ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # 5. Extract the 'rouge1', 'rouge2', 'rougeL' scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T14:15:00.941015Z",
     "iopub.status.busy": "2025-10-31T14:15:00.940816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upscaled training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mruksad/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7548' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7548/37500 1:32:22 < 6:06:38, 1.36 it/s, Epoch 0.60/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.182427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.91it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.72it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.25it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.09it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.06it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.48it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.34it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.50it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.27it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.46 GiB, other allocations: 11.50 GiB, max allowed: 30.19 GiB). Tried to allocate 256.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# 4. Start Training! ðŸš€\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# This will take 1-2 hours on a Kaggle GPU, but it will not stop.\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting upscaled training...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTraining complete!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     43\u001B[39m \u001B[38;5;66;03m# 5. Save your final model\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/transformers/trainer.py:2170\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2168\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2169\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2170\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2171\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2172\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2173\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2174\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2175\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/transformers/trainer.py:2537\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2530\u001B[39m context = (\n\u001B[32m   2531\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2532\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2533\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2534\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2535\u001B[39m )\n\u001B[32m   2536\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2537\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2539\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2540\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2541\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2542\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2543\u001B[39m ):\n\u001B[32m   2544\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2545\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/transformers/trainer.py:3838\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(***failed resolving arguments***)\u001B[39m\n\u001B[32m   3835\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001B[32m   3836\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mscale_wrt_gas\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3838\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3840\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss.detach()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/accelerate/accelerator.py:2848\u001B[39m, in \u001B[36mAccelerator.backward\u001B[39m\u001B[34m(self, loss, **kwargs)\u001B[39m\n\u001B[32m   2846\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m   2847\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.scaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2848\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2849\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m learning_rate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.has_lomo_optimizer:\n\u001B[32m   2850\u001B[39m     \u001B[38;5;28mself\u001B[39m.lomo_backward(loss, learning_rate)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/torch/_tensor.py:630\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    620\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    622\u001B[39m         Tensor.backward,\n\u001B[32m    623\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    628\u001B[39m         inputs=inputs,\n\u001B[32m    629\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/torch/autograd/__init__.py:364\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    359\u001B[39m     retain_graph = create_graph\n\u001B[32m    361\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    362\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    363\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    369\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/IIT-B/epgd/second-sem/GL-selflearning-sem2/abstrative-text-summarisation/.local/lib/python3.14/site-packages/torch/autograd/graph.py:865\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    863\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    864\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m865\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    866\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    867\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    869\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 18.46 GiB, other allocations: 11.50 GiB, max allowed: 30.19 GiB). Tried to allocate 256.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# 1. Define the Training Arguments for the upscaled run\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_t5_summarizer_upscaled\", # Use a new directory\n",
    "    \n",
    "    # --- Critical Changes for Upscaling ---\n",
    "    eval_strategy=\"no\",                   # 1. DO NOT evaluate during training\n",
    "    report_to=\"none\",                     # 2. Keep our fix from before\n",
    "    logging_steps=200,                    # 3. Just log progress\n",
    "    # -------------------------------------\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=8,    \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,              \n",
    "    predict_with_generate=True,      \n",
    "    fp16=True,                       \n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# 2. Create the Data Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 3. Initialize the Trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  \n",
    "    eval_dataset=tokenized_eval_dataset,    \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,      \n",
    ")\n",
    "\n",
    "# 4. Start Training! ðŸš€\n",
    "# This will take 1-2 hours on a Kaggle GPU, but it will not stop.\n",
    "print(\"Starting upscaled training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 5. Save your final model\n",
    "trainer.save_model(\"my_final_t5_model_upscaled\")\n",
    "print(\"Upscaled model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:12:27.717889Z",
     "iopub.status.busy": "2025-10-31T13:12:27.717064Z",
     "iopub.status.idle": "2025-10-31T13:13:05.739000Z",
     "shell.execute_reply": "2025-10-31T13:13:05.738215Z",
     "shell.execute_reply.started": "2025-10-31T13:12:27.717862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation on the 500 validation examples...\n",
      "This may take 5-15 minutes as it generates summaries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL ROUGE SCORES ---\n",
      "{'eval_loss': 1.667940378189087, 'eval_rouge1': 25.9107, 'eval_rouge2': 12.897, 'eval_rougeL': 21.4397, 'eval_rougeLsum': 24.4141, 'eval_runtime': 38.0104, 'eval_samples_per_second': 13.154, 'eval_steps_per_second': 0.842, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# --- NEW STEP: Run Final Evaluation ---\n",
    "\n",
    "print(\"Running final evaluation on the 500 validation examples...\")\n",
    "print(\"This may take 5-15 minutes as it generates summaries.\")\n",
    "\n",
    "# This command tells the trainer to run the evaluation\n",
    "# on the 'eval_dataset' one time.\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\n--- FINAL ROUGE SCORES ---\")\n",
    "print(metrics)\n",
    "\n",
    "# This will save your scores to a file for your report\n",
    "import json\n",
    "with open(\"upscaled_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T14:05:41.369237Z",
     "iopub.status.busy": "2025-10-31T14:05:41.368932Z",
     "iopub.status.idle": "2025-10-31T14:05:45.149720Z",
     "shell.execute_reply": "2025-10-31T14:05:45.148943Z",
     "shell.execute_reply.started": "2025-10-31T14:05:41.369214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Lead-3 summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaf039172904ceea949c1579a908dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeecfca9d13e4dc7be1d5a8c0b1ee347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LEAD-3 BASELINE ROUGE SCORES ---\n",
      "{'rouge1': 41.8318, 'rouge2': 19.3325, 'rougeL': 26.4416, 'rougeLsum': 38.1535}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm # Shows a progress bar\n",
    "\n",
    "nltk.download('punkt') # Make sure the sentence tokenizer is downloaded\n",
    "\n",
    "# Load your (non-tokenized) eval dataset again\n",
    "eval_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# 1. The Lead-3 Function\n",
    "def lead3_summarizer(article):\n",
    "    # Split into sentences\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    # Get the first 3\n",
    "    lead_3 = \" \".join(sentences[:3])\n",
    "    return lead_3\n",
    "\n",
    "# 2. Get all predictions and labels\n",
    "print(\"Generating Lead-3 summaries...\")\n",
    "lead_3_predictions = [lead3_summarizer(article) for article in tqdm(eval_dataset[\"article\"])]\n",
    "human_labels = [summary for summary in tqdm(eval_dataset[\"highlights\"])]\n",
    "\n",
    "# 3. Get the ROUGE score\n",
    "# We can't use the full 'compute_metrics' function as-is \n",
    "# because it's designed for the Trainer. We'll call the metric directly.\n",
    "\n",
    "# This is the same ROUGE metric from your Step 5\n",
    "from evaluate import load\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Prepare for ROUGE (it likes newlines between sentences)\n",
    "decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in lead_3_predictions]\n",
    "decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in human_labels]\n",
    "\n",
    "# Calculate ROUGE\n",
    "result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "result = {key: value * 100 for key, value in result.items()}\n",
    "result = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "print(\"\\n--- LEAD-3 BASELINE ROUGE SCORES ---\")\n",
    "print(result)\n",
    "\n",
    "# Your T5-Small model (25.9 ROUGE-1) is now \"Baseline 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T13:39:26.995072Z",
     "iopub.status.busy": "2025-10-31T13:39:26.994346Z",
     "iopub.status.idle": "2025-10-31T13:39:28.412951Z",
     "shell.execute_reply": "2025-10-31T13:39:28.412198Z",
     "shell.execute_reply.started": "2025-10-31T13:39:26.995045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ORIGINAL ARTICLE ---\n",
      "In a significant advancement for artificial intelligence, researchers today announced the development of a new algorithm that can learn from vastly smaller datasets. This breakthrough, named 'Sparse Learning', could democratize AI by allowing smaller companies and individuals to build powerful models without the need for massive computational resources. The algorithm works by identifying and focusing on the most critical pieces of information, ignoring redundant data, which leads to faster training times and reduced computational cost.\n",
      "\n",
      "--- GENERATED SUMMARY ---\n",
      "This breakthrough could democratize AI by allowing smaller companies and individuals to build powerful models without the need for massive computational resources. The algorithm works by identifying and focusing on the most critical pieces of information, ignoring redundant data, which leads to faster training times and reduced computational cost.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load your fine-tuned model and tokenizer\n",
    "# This is the directory where 'trainer.save_model()' saved your files\n",
    "model_path = \"my_final_t5_model_upscaled\" \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device) # Move model to GPU\n",
    "\n",
    "# 2. Define a new, unseen article to summarize\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"In a significant advancement for artificial intelligence, researchers today \"\n",
    "    \"announced the development of a new algorithm that can learn from \"\n",
    "    \"vastly smaller datasets. This breakthrough, named 'Sparse Learning', \"\n",
    "    \"could democratize AI by allowing smaller companies and individuals to \"\n",
    "    \"build powerful models without the need for massive computational resources. \"\n",
    "    \"The algorithm works by identifying and focusing on the most critical \"\n",
    "    \"pieces of information, ignoring redundant data, which leads to faster \"\n",
    "    \"training times and reduced computational cost.\"\n",
    ")\n",
    "\n",
    "# 3. Prepare the input text (add the T5 prefix)\n",
    "text = \"summarize: \" + ARTICLE_TO_SUMMARIZE\n",
    "\n",
    "# 4. Tokenize the text and move tensors to the GPU\n",
    "inputs = tokenizer(\n",
    "    text, \n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    max_length=1024,      # Max input length (matches what we trained on)\n",
    "    truncation=True\n",
    ").to(device) # Move the input tensors to the GPU\n",
    "\n",
    "# 5. Generate the summary\n",
    "# model.generate() is the main function for inference\n",
    "# We use 'num_beams' for beam search to get a better quality summary\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    num_beams=4,       # Number of \"paths\" to explore\n",
    "    max_length=150,    # Set a max length for the output summary\n",
    "    min_length=30,     # Set a min length\n",
    "    early_stopping=True # Stop when the model is confident the summary is done\n",
    ")\n",
    "\n",
    "# 6. Decode the generated IDs back to text\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 7. Print the results\n",
    "print(\"\\n--- ORIGINAL ARTICLE ---\")\n",
    "print(ARTICLE_TO_SUMMARIZE)\n",
    "print(\"\\n--- GENERATED SUMMARY ---\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1654566,
     "sourceId": 2734496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
